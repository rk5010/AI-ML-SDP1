{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore harmless warnings \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set to display all the columns in dataset\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Import psql to run queries \n",
    "\n",
    "import pandasql as psql\n",
    "\n",
    "# import datetime class from datetime module\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  fractal_dimension_mean  radius_se  texture_se  perimeter_se  \\\n",
       "0         0.2419                 0.07871     1.0950      0.9053         8.589   \n",
       "1         0.1812                 0.05667     0.5435      0.7339         3.398   \n",
       "2         0.2069                 0.05999     0.7456      0.7869         4.585   \n",
       "3         0.2597                 0.09744     0.4956      1.1560         3.445   \n",
       "4         0.1809                 0.05883     0.7572      0.7813         5.438   \n",
       "\n",
       "   area_se  smoothness_se  compactness_se  concavity_se  concave points_se  \\\n",
       "0   153.40       0.006399         0.04904       0.05373            0.01587   \n",
       "1    74.08       0.005225         0.01308       0.01860            0.01340   \n",
       "2    94.03       0.006150         0.04006       0.03832            0.02058   \n",
       "3    27.23       0.009110         0.07458       0.05661            0.01867   \n",
       "4    94.44       0.011490         0.02461       0.05688            0.01885   \n",
       "\n",
       "   symmetry_se  fractal_dimension_se  radius_worst  texture_worst  \\\n",
       "0      0.03003              0.006193         25.38          17.33   \n",
       "1      0.01389              0.003532         24.99          23.41   \n",
       "2      0.02250              0.004571         23.57          25.53   \n",
       "3      0.05963              0.009208         14.91          26.50   \n",
       "4      0.01756              0.005115         22.54          16.67   \n",
       "\n",
       "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0           184.60      2019.0            0.1622             0.6656   \n",
       "1           158.80      1956.0            0.1238             0.1866   \n",
       "2           152.50      1709.0            0.1444             0.4245   \n",
       "3            98.87       567.7            0.2098             0.8663   \n",
       "4           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0           0.7119                0.2654          0.4601   \n",
       "1           0.2416                0.1860          0.2750   \n",
       "2           0.4504                0.2430          0.3613   \n",
       "3           0.6869                0.2575          0.6638   \n",
       "4           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Breast cancer dataset\n",
    "\n",
    "bcdata = pd.read_csv(r\"D:\\00 Datasets\\Others\\Data-06\\breast_cancer.csv\", header=0)\n",
    "bcdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display the dataset information\n",
    "\n",
    "bcdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         569\n",
       "diagnosis                    2\n",
       "radius_mean                456\n",
       "texture_mean               479\n",
       "perimeter_mean             522\n",
       "area_mean                  539\n",
       "smoothness_mean            474\n",
       "compactness_mean           537\n",
       "concavity_mean             537\n",
       "concave points_mean        542\n",
       "symmetry_mean              432\n",
       "fractal_dimension_mean     499\n",
       "radius_se                  540\n",
       "texture_se                 519\n",
       "perimeter_se               533\n",
       "area_se                    528\n",
       "smoothness_se              547\n",
       "compactness_se             541\n",
       "concavity_se               533\n",
       "concave points_se          507\n",
       "symmetry_se                498\n",
       "fractal_dimension_se       545\n",
       "radius_worst               457\n",
       "texture_worst              511\n",
       "perimeter_worst            514\n",
       "area_worst                 544\n",
       "smoothness_worst           411\n",
       "compactness_worst          529\n",
       "concavity_worst            539\n",
       "concave points_worst       492\n",
       "symmetry_worst             500\n",
       "fractal_dimension_worst    535\n",
       "Unnamed: 32                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the # of unique values for each variable\n",
    "\n",
    "bcdata.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display all the columns names of dataset\n",
    "\n",
    "bcdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the variables which are not impacting the target variable\n",
    "\n",
    "del bcdata['Unnamed: 32']\n",
    "del bcdata['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the independent and Target variables\n",
    "\n",
    "IndepVar = []\n",
    "for col in bcdata.columns:\n",
    "    if col != 'diagnosis':\n",
    "        IndepVar.append(col)\n",
    "\n",
    "TargetVar = 'diagnosis'\n",
    "\n",
    "x = bcdata[IndepVar]\n",
    "y = bcdata[TargetVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#creating labelEncoder\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "\n",
    "y=le.fit_transform(y)\n",
    "y=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state = 42)\n",
    "\n",
    "# Copy the test data to back-up file\n",
    "\n",
    "x_test_F1 = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_train = pd.DataFrame(x_train_scaled)\n",
    "\n",
    "x_test_scaled = scaler.fit_transform(x_test)\n",
    "x_test = pd.DataFrame(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model Name, Accuracy, Precision, Recall, F1 Score, Specificity, MCC, ROC_AUC_Score, Balanced Accuracy]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the result dataset\n",
    "\n",
    "CSResults = pd.read_csv(r\"D:\\000 DataScience\\01-Internship\\CSResults.csv\", header=0)\n",
    "CSResults.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'min_impurity_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaggingClassifier\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m----> 8\u001b[0m modelDT \u001b[38;5;241m=\u001b[39m \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgini\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_weight_fraction_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_leaf_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_impurity_decrease\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmin_impurity_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mccp_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m modelBAG \u001b[38;5;241m=\u001b[39m BaggingClassifier(base_estimator\u001b[38;5;241m=\u001b[39mmodelDT, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     14\u001b[0m                              bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bootstrap_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, oob_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m                              n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Fit the model with train data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'min_impurity_split'"
     ]
    }
   ],
   "source": [
    "# Training bagging classifier - BaggingClassifier class of 'sklearn.ensemble' packages to build bagging classifier model. \n",
    "# We set DecisionTreeClassifier class as a base estimator and set 100 to the number of estimators, then train the model \n",
    "# with train data.  \n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "modelDT = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2,\n",
    "                                 min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                                 random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                 class_weight=None, ccp_alpha=0.0)\n",
    "\n",
    "modelBAG = BaggingClassifier(base_estimator=modelDT, n_estimators=200, max_samples=1.0, max_features=1.0,\n",
    "                             bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False,\n",
    "                             n_jobs=None, random_state=None, verbose=0)\n",
    "\n",
    "# Fit the model with train data\n",
    "\n",
    "modelBAG.fit(x_train,y_train)\n",
    "\n",
    "# Predict the model with test data set\n",
    "\n",
    "y_pred = modelBAG.predict(x_test)\n",
    "y_pred_prob = modelBAG.predict_proba(x_test)\n",
    "\n",
    "# Confusion matrix in sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# actual values\n",
    "\n",
    "actual = y_test\n",
    "\n",
    "# predicted values\n",
    "\n",
    "predicted = y_pred\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "# outcome values order in sklearn\n",
    "\n",
    "tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "# classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "C_Report = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "print('Classification report : \\n', C_Report)\n",
    "\n",
    "# calculating the metrics\n",
    "\n",
    "sensitivity = round(tp/(tp+fn), 3);\n",
    "specificity = round(tn/(tn+fp), 3);\n",
    "accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "precision = round(tp/(tp+fp), 3);\n",
    "f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "# A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "print('Precision :', round(precision*100, 2),'%')\n",
    "print('Recall :', round(sensitivity*100,2), '%')\n",
    "print('F1 Score :', f1Score)\n",
    "print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )\n",
    "print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "print('MCC :', MCC)\n",
    "\n",
    "# Area under ROC curve \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))\n",
    "\n",
    "# ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,modelBAG.predict_proba(x_test)[:,1])\n",
    "plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot(fpr, tpr, label= 'Classification Model' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show() \n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "#-------------------------------------------------------------------------------------\n",
    "new_row = {'Model Name' : modelBAG,\n",
    "           'Accuracy' : accuracy,\n",
    "           'Precision' : precision,\n",
    "           'Recall' : sensitivity,\n",
    "           'F1 Score' : f1Score,\n",
    "           'Specificity' : specificity,\n",
    "           'MCC':MCC,\n",
    "           'ROC_AUC_Score':roc_auc_score(y_test, y_pred),\n",
    "           'Balanced Accuracy':balanced_accuracy}\n",
    "CSResults = CSResults.append(new_row, ignore_index=True)\n",
    "#-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy : 88.1 %\n",
    "Precision : 76.1 %\n",
    "Recall : 100.0 %\n",
    "F1 Score : 0.864\n",
    "Specificity or True Negative Rate : 80.9 %\n",
    "Balanced Accuracy : 90.5 %\n",
    "MCC : 0.784\n",
    "roc_auc_score: 0.904\n",
    "    \n",
    "Accuracy : 91.6 %\n",
    "Precision : 81.8 %\n",
    "Recall : 100.0 %\n",
    "F1 Score : 0.9\n",
    "Specificity or True Negative Rate : 86.5 %\n",
    "Balanced Accuracy : 93.2 %\n",
    "MCC : 0.841\n",
    "roc_auc_score: 0.933\n",
    "    \n",
    "Accuracy : 88.1 %\n",
    "Precision : 76.1 %\n",
    "Recall : 100.0 %\n",
    "F1 Score : 0.864\n",
    "Specificity or True Negative Rate : 80.9 %\n",
    "Balanced Accuracy : 90.5 %\n",
    "MCC : 0.784\n",
    "roc_auc_score: 0.904\n",
    "    \n",
    "Accuracy : 89.5 %\n",
    "Precision : 78.3 %\n",
    "Recall : 100.0 %\n",
    "F1 Score : 0.878\n",
    "Specificity or True Negative Rate : 83.1 %\n",
    "Balanced Accuracy : 91.6 %\n",
    "MCC : 0.807\n",
    "roc_auc_score: 0.916"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier with other classification models as base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  None\n",
      "Confusion matrix : \n",
      " [[53  1]\n",
      " [14 75]]\n",
      "Outcome values : \n",
      " 53 1 14 75\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.98      0.88        54\n",
      "           0       0.99      0.84      0.91        89\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.89      0.91      0.89       143\n",
      "weighted avg       0.91      0.90      0.90       143\n",
      "\n",
      "Accuracy : 89.5 %\n",
      "Precision : 79.1 %\n",
      "Recall : 98.1 %\n",
      "F1 Score : 0.876\n",
      "Balanced Accuracy : 91.2 %\n",
      "MCC : 0.801\n",
      "roc_auc_score: 0.912\n",
      "-----------------------------------------------------------------------\n",
      "Method:  LogisticRegression()\n",
      "Confusion matrix : \n",
      " [[53  1]\n",
      " [ 7 82]]\n",
      "Outcome values : \n",
      " 53 1 7 82\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.98      0.93        54\n",
      "           0       0.99      0.92      0.95        89\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.95      0.94       143\n",
      "weighted avg       0.95      0.94      0.94       143\n",
      "\n",
      "Accuracy : 94.4 %\n",
      "Precision : 88.3 %\n",
      "Recall : 98.1 %\n",
      "F1 Score : 0.93\n",
      "Balanced Accuracy : 95.1 %\n",
      "MCC : 0.887\n",
      "roc_auc_score: 0.951\n",
      "-----------------------------------------------------------------------\n",
      "Method:  RandomForestClassifier(n_estimators=500, random_state=0)\n",
      "Confusion matrix : \n",
      " [[54  0]\n",
      " [13 76]]\n",
      "Outcome values : \n",
      " 54 0 13 76\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      1.00      0.89        54\n",
      "           0       1.00      0.85      0.92        89\n",
      "\n",
      "    accuracy                           0.91       143\n",
      "   macro avg       0.90      0.93      0.91       143\n",
      "weighted avg       0.93      0.91      0.91       143\n",
      "\n",
      "Accuracy : 90.9 %\n",
      "Precision : 80.6 %\n",
      "Recall : 100.0 %\n",
      "F1 Score : 0.893\n",
      "Balanced Accuracy : 92.7 %\n",
      "MCC : 0.83\n",
      "roc_auc_score: 0.927\n",
      "-----------------------------------------------------------------------\n",
      "Method:  DecisionTreeClassifier(criterion='entropy')\n",
      "Confusion matrix : \n",
      " [[54  0]\n",
      " [14 75]]\n",
      "Outcome values : \n",
      " 54 0 14 75\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      1.00      0.89        54\n",
      "           0       1.00      0.84      0.91        89\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.90      0.92      0.90       143\n",
      "weighted avg       0.92      0.90      0.90       143\n",
      "\n",
      "Accuracy : 90.2 %\n",
      "Precision : 79.4 %\n",
      "Recall : 100.0 %\n",
      "F1 Score : 0.885\n",
      "Balanced Accuracy : 92.2 %\n",
      "MCC : 0.818\n",
      "roc_auc_score: 0.921\n",
      "-----------------------------------------------------------------------\n",
      "Method:  SVC(probability=True)\n",
      "Confusion matrix : \n",
      " [[53  1]\n",
      " [13 76]]\n",
      "Outcome values : \n",
      " 53 1 13 76\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.98      0.88        54\n",
      "           0       0.99      0.85      0.92        89\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.90      0.92      0.90       143\n",
      "weighted avg       0.92      0.90      0.90       143\n",
      "\n",
      "Accuracy : 90.2 %\n",
      "Precision : 80.3 %\n",
      "Recall : 98.1 %\n",
      "F1 Score : 0.883\n",
      "Balanced Accuracy : 91.8 %\n",
      "MCC : 0.812\n",
      "roc_auc_score: 0.918\n",
      "-----------------------------------------------------------------------\n",
      "Method:  KNeighborsClassifier()\n",
      "Confusion matrix : \n",
      " [[53  1]\n",
      " [ 5 84]]\n",
      "Outcome values : \n",
      " 53 1 5 84\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.98      0.95        54\n",
      "           0       0.99      0.94      0.97        89\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.95      0.96      0.96       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n",
      "Accuracy : 95.8 %\n",
      "Precision : 91.4 %\n",
      "Recall : 98.1 %\n",
      "F1 Score : 0.946\n",
      "Balanced Accuracy : 96.2 %\n",
      "MCC : 0.914\n",
      "roc_auc_score: 0.963\n",
      "-----------------------------------------------------------------------\n",
      "Method:  GradientBoostingClassifier()\n",
      "Confusion matrix : \n",
      " [[54  0]\n",
      " [14 75]]\n",
      "Outcome values : \n",
      " 54 0 14 75\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      1.00      0.89        54\n",
      "           0       1.00      0.84      0.91        89\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.90      0.92      0.90       143\n",
      "weighted avg       0.92      0.90      0.90       143\n",
      "\n",
      "Accuracy : 90.2 %\n",
      "Precision : 79.4 %\n",
      "Recall : 100.0 %\n",
      "F1 Score : 0.885\n",
      "Balanced Accuracy : 92.2 %\n",
      "MCC : 0.818\n",
      "roc_auc_score: 0.921\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy by changing base estimator - \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "modelLR = LogisticRegression()\n",
    "modelRF = RandomForestClassifier(criterion='gini', n_estimators=500, random_state=0)\n",
    "modelDT = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "modelSVMGaussian = SVC(kernel='rbf', random_state = None, class_weight=None,probability=True)\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30,\n",
    "                                p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "modelXGB = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100,\n",
    "                                      subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,\n",
    "                                      min_impurity_decrease=0.0, min_impurity_split=None, init=None,\n",
    "                                      random_state=None, max_features=None, verbose=0, max_leaf_nodes=None,\n",
    "                                      warm_start=False, validation_fraction=0.1, n_iter_no_change=None,\n",
    "                                      tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "base_methods=[None, modelLR, modelRF, modelDT, modelSVMGaussian, modelKNN, modelXGB]\n",
    "for bm in base_methods:\n",
    "    print(\"Method: \", bm)\n",
    "    modelBAG = BaggingClassifier(base_estimator=bm,n_estimators=100,bootstrap=True)\n",
    "    \n",
    "    # fit the model with train data\n",
    "    \n",
    "    modelBAG.fit(x_train, y_train)\n",
    "    \n",
    "    # Predict the model\n",
    "    \n",
    "    y_pred = modelBAG.predict(x_test)\n",
    "    \n",
    "    # Evaluate the model performance by metrics\n",
    "    # confusion matrix in sklearn\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # actual values\n",
    "\n",
    "    actual = y_test\n",
    "\n",
    "    # predicted values\n",
    "\n",
    "    predicted = y_pred\n",
    "\n",
    "    # confusion matrix\n",
    "\n",
    "    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)\n",
    "    print('Confusion matrix : \\n', matrix)\n",
    "\n",
    "    # outcome values order in sklearn\n",
    "\n",
    "    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "    print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "    # classification report for precision, recall f1-score and accuracy\n",
    "\n",
    "    matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "\n",
    "    print('Classification report : \\n',matrix)\n",
    "\n",
    "    # calculating the metrics\n",
    "\n",
    "    sensitivity = round(tp/(tp+fn), 3)\n",
    "    specificity = round(tn/(tn+fp), 3);\n",
    "    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);\n",
    "    balanced_accuracy = round((sensitivity+specificity)/2, 3);\n",
    "    \n",
    "    precision = round(tp/(tp+fp), 3);\n",
    "    f1Score = round((2*tp/(2*tp + fp + fn)), 3);\n",
    "\n",
    "    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. \n",
    "    # A model with a score of +1 is a perfect model and -1 is a poor model\n",
    "\n",
    "    from math import sqrt\n",
    "\n",
    "    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)\n",
    "    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)\n",
    "\n",
    "    print('Accuracy :', round(accuracy*100, 2),'%')\n",
    "    print('Precision :', round(precision*100, 2),'%')\n",
    "    print('Recall :', round(sensitivity*100,2), '%')\n",
    "    print('F1 Score :', f1Score)\n",
    "    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')\n",
    "    print('MCC :', MCC)\n",
    "\n",
    "    # Area under ROC curve \n",
    "\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))\n",
    "    print('-----------------------------------------------------------------------')\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    new_row = {'Model Name' : bm,\n",
    "               'Accuracy' : accuracy,\n",
    "               'Precision' : precision,\n",
    "               'Recall' : sensitivity,\n",
    "               'F1 Score' : f1Score,\n",
    "               'Specificity' : specificity,\n",
    "               'MCC':MCC,\n",
    "               'ROC_AUC_Score':roc_auc_score(y_test, y_pred),\n",
    "               'Balanced Accuracy':balanced_accuracy}\n",
    "    CSResults = CSResults.append(new_row, ignore_index=True)\n",
    "    #-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(DecisionTreeClassifier(random_state=175107625...</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.801</td>\n",
       "      <td>0.912089</td>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.951415</td>\n",
       "      <td>0.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier(n_estimators=500, rando...</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.806</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeClassifier(criterion='entropy')</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.794</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC(probability=True)</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.917707</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsClassifier()</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.962651</td>\n",
       "      <td>0.962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.794</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Model Name  Accuracy  Precision  \\\n",
       "0  (DecisionTreeClassifier(random_state=175107625...     0.874      0.750   \n",
       "1                                                NaN     0.895      0.791   \n",
       "2                               LogisticRegression()     0.944      0.883   \n",
       "3  RandomForestClassifier(n_estimators=500, rando...     0.909      0.806   \n",
       "4        DecisionTreeClassifier(criterion='entropy')     0.902      0.794   \n",
       "5                              SVC(probability=True)     0.902      0.803   \n",
       "6                             KNeighborsClassifier()     0.958      0.914   \n",
       "7                       GradientBoostingClassifier()     0.902      0.794   \n",
       "\n",
       "   Recall  F1 Score  Specificity    MCC  ROC_AUC_Score  Balanced Accuracy  \n",
       "0   1.000     0.857        0.798  0.774       0.898876              0.899  \n",
       "1   0.981     0.876        0.843  0.801       0.912089              0.912  \n",
       "2   0.981     0.930        0.921  0.887       0.951415              0.951  \n",
       "3   1.000     0.893        0.854  0.830       0.926966              0.927  \n",
       "4   1.000     0.885        0.843  0.818       0.921348              0.922  \n",
       "5   0.981     0.883        0.854  0.812       0.917707              0.918  \n",
       "6   0.981     0.946        0.944  0.914       0.962651              0.962  \n",
       "7   1.000     0.885        0.843  0.818       0.921348              0.922  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results with comparing the all the algorithms \n",
    "\n",
    "#CSResults.to_csv(\"D://000 DataScience//01-Internship//CSResults_50.csv\")\n",
    "\n",
    "CSResults.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
